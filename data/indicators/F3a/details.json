{"id":"F03a","name":"F3a","category":"freedom","label":"Process for terms of service enforcement","description":"The company should clearly disclose the circumstances under which it may restrict content or user accounts.","guidance":"It is fair to expect companies to set rules prohibiting certain content or activities—like toxic speech or malicious behavior. However, when companies develop and enforce rules about what people can do and say on the internet—or whether they can access a service at all—they must do so in a way that is transparent and accountable. We therefore expect companies to clearly disclose what these rules are and how they enforce them. This includes information about how companies learn of material or activities that violate their terms. For example, companies may rely on outside contractors to review content and/or user activity. They may also rely on community flagging mechanisms that allow users to flag other users' content and/or activity for company review. They may also deploy algorithmic systems to detect and flag breaches, in which case, companies should explain how these systems are used and on what types of content. We expect companies to clearly disclose whether they have a policy of granting priority or expedited consideration to any government authorities and/or members of private organizations or other entities that identify their organizational affiliation when they report content or users for allegedly violating the company's rules. For mobile ecosystems, we expect companies to disclose the types of apps they would restrict. For personal digital assistant ecosystems, we expect companies to disclose the types of skills and search results they would restrict. In this disclosure, the company should also provide examples to help users understand what these rules mean.","isParent":false,"hasParent":true}